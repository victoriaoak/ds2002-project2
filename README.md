# ds2002-project2
**Name:** Victoria Ok (vyo7tv) <br>
**Database Used:** [Sakila](https://dev.mysql.com/doc/sakila/en/) <br>
**Purpose:** to demonstrate how to implement Data Lakehouse Architecture using Databricks Spark Structured Streaming and Delta Tables technologies. This project uses  different types of data systems and an Extract-Transform-Load (ETL) pipeline that is optimized for post hoc diagnostic analysis. This project utilizes the Sakila database to do this, which represents a business process of the interaction between customers and retailers (staff) that is reflected in movie rental transactions. <br>
**Data:** (exported from SQL)
  - CSV files (.sv)
  - JSON files (.json)

<br>
Note: outside sources and code from project scripts/demos were also used

### Azure Resources Used: SQL Server and Databricks
![image](https://user-images.githubusercontent.com/89366891/207525234-e3bfc469-5fa5-47c2-baa6-c9d712b9773b.png)

### MongoDB Resources Used: Compass and Atlas
![image](https://user-images.githubusercontent.com/89366891/207524767-e0fe51f2-5f69-4a02-b035-07b29bc5486d.png)
![image](https://user-images.githubusercontent.com/89366891/207524583-7c9f68a9-ef03-4b01-9a91-dceaea7cc585.png)

### Pivot Table Screenshot
![image](https://user-images.githubusercontent.com/89366891/207526677-9b511751-a94e-4438-bd14-c5729c8697d4.png)
